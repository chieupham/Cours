{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "7_TFLite_Pruning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L92sp0x6fieL"
      },
      "source": [
        "## Download packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0pZJRr977Vs",
        "outputId": "0b0f7052-6420-42c7-852d-64199ca2ba5e"
      },
      "source": [
        "!pip uninstall -y tensorflow\r\n",
        "!pip install -q tf-nightly-gpu\r\n",
        "!pip install -q tensorflow-model-optimization==0.5\r\n",
        "!pip install tensorflow==2.4.0\r\n",
        "\r\n",
        "import tempfile\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from tensorflow import keras\r\n",
        "\r\n",
        "import tensorflow_model_optimization as tfmot\r\n",
        "\r\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.4.0:\n",
            "  Successfully uninstalled tensorflow-2.4.0\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed.\u001b[0m\n",
            "Collecting tensorflow==2.4.0\n",
            "  Using cached https://files.pythonhosted.org/packages/94/0a/012cc33c643d844433d13001dd1db179e7020b05ddbbd0a9dc86c38a8efa/tensorflow-2.4.0-cp37-cp37m-manylinux2010_x86_64.whl\n",
            "Collecting gast==0.3.3\n",
            "  Using cached https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.3.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.36.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.12)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.10.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.2)\n",
            "Collecting grpcio~=1.32.0\n",
            "  Using cached https://files.pythonhosted.org/packages/06/54/1c8be62beafe7fb1548d2968e518ca040556b46b0275399d4f3186c56d79/grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl\n",
            "Collecting h5py~=2.10.0\n",
            "  Using cached https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.7.4.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.12.1)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (2.4.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.12.4)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (2.4.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.6.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.15.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (54.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (1.24.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (3.1.0)\n",
            "\u001b[31mERROR: tf-nightly-gpu 2.5.0.dev20210318 has requirement gast==0.4.0, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tf-nightly-gpu 2.5.0.dev20210318 has requirement grpcio~=1.34.0, but you'll have grpcio 1.32.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tf-nightly-gpu 2.5.0.dev20210318 has requirement h5py~=3.1.0, but you'll have h5py 2.10.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, grpcio, h5py, tensorflow\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: grpcio 1.34.1\n",
            "    Uninstalling grpcio-1.34.1:\n",
            "      Successfully uninstalled grpcio-1.34.1\n",
            "  Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "Successfully installed gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 tensorflow-2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riDrC6m1flwN"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yaE4wS__3Ao",
        "outputId": "38449124-1269-421b-fe3d-ed73f8299ccb"
      },
      "source": [
        "import tensorflow_datasets as tfds\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "BATCH_SIZE = 32\r\n",
        "IMAGE_SIZE = 224\r\n",
        "\r\n",
        "def format_image(image, label):\r\n",
        "    image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE)) / 255.0\r\n",
        "    return  image, label\r\n",
        "\r\n",
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\r\n",
        "    'cats_vs_dogs',\r\n",
        "    data_dir='/content/dataset/',\r\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\r\n",
        "    with_info=True,\r\n",
        "    shuffle_files=True, \r\n",
        "    as_supervised=True,\r\n",
        ")\r\n",
        "\r\n",
        "# tf.data.experimental.cardinality(raw_train_dataset).numpy()\r\n",
        "num_examples = metadata.splits['train'].num_examples\r\n",
        "num_classes = metadata.features['label'].num_classes\r\n",
        "print(\"Numbers of images: \", num_examples)\r\n",
        "print(\"Numbers of classes: \", num_classes)\r\n",
        "\r\n",
        "train_batches = raw_train.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE).prefetch(1)\r\n",
        "validation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(1)\r\n",
        "test_batches = raw_test.map(format_image).batch(1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numbers of images:  23262\n",
            "Numbers of classes:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw5QVDuHfpDR"
      },
      "source": [
        "## Build a Keras model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "318siK7H_6sG",
        "outputId": "1b0e95c8-3e2a-4769-e330-cec161bf0cfc"
      },
      "source": [
        "model = tf.keras.models.Sequential([\r\n",
        "    # Note the input shape is the desired size of the image with 3 bytes color\r\n",
        "    # This is the first convolution\r\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),\r\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "    # The second convolution\r\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    # The third convolution\r\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    # The fourth convolution\r\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    # Flatten the results to feed into a DNN\r\n",
        "    tf.keras.layers.Flatten(),\r\n",
        "    # 512 neuron hidden layer\r\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\r\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "    # Output neuron\r\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\r\n",
        "])\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "from tensorflow.keras.optimizers import RMSprop\r\n",
        "model.compile(loss='sparse_categorical_crossentropy',\r\n",
        "              optimizer=RMSprop(lr=0.001),\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 222, 222, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 111, 111, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 109, 109, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 54, 54, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 52, 52, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 26, 26, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 24, 24, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               2359552   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 2,453,218\n",
            "Trainable params: 2,453,218\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJBraRSHftgF"
      },
      "source": [
        "## Training our model\r\n",
        "\r\n",
        "We will train our model within 3 epoches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIrsBu8o_9qU",
        "outputId": "d3e44356-9ad5-404f-a8f7-423ba423c85b"
      },
      "source": [
        "history = model.fit(\r\n",
        "      train_batches,  \r\n",
        "      epochs=3,\r\n",
        "      validation_data=validation_batches)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "582/582 [==============================] - 37s 49ms/step - loss: 0.6751 - acc: 0.5959 - val_loss: 0.5292 - val_acc: 0.7532\n",
            "Epoch 2/3\n",
            "582/582 [==============================] - 31s 47ms/step - loss: 0.5128 - acc: 0.7547 - val_loss: 0.5003 - val_acc: 0.7554\n",
            "Epoch 3/3\n",
            "582/582 [==============================] - 31s 47ms/step - loss: 0.4398 - acc: 0.7977 - val_loss: 0.4631 - val_acc: 0.7966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn--rXURfz7Z"
      },
      "source": [
        "##Fine-tune pre-trained model with pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtjwdjMAanUM",
        "outputId": "06a198f6-19a7-4b7b-b50d-b76e06e2fdc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\r\n",
        "\r\n",
        "# Compute end step to finish pruning after 2 epochs.\r\n",
        "epochs = 2\r\n",
        "end_step = np.ceil(num_examples*0.8 / BATCH_SIZE).astype(np.int32) * epochs\r\n",
        "\r\n",
        "# Define model for pruning.\r\n",
        "pruning_params = {\r\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\r\n",
        "                                                               final_sparsity=0.80,\r\n",
        "                                                               begin_step=0,\r\n",
        "                                                               end_step=end_step)\r\n",
        "}\r\n",
        "\r\n",
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)\r\n",
        "# `prune_low_magnitude` requires a recompile.\r\n",
        "model_for_pruning.compile(optimizer='adam',\r\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "model_for_pruning.summary()\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:2281: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_conv2d ( (None, 222, 222, 16)      882       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 111, 111, 16)      1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 109, 109, 32)      9250      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 54, 54, 32)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_2 (None, 52, 52, 64)        36930     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 26, 26, 64)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_3 (None, 24, 24, 64)        73794     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 12, 12, 64)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_flatten  (None, 9216)              1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense (P (None, 256)               4718850   \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_1  (None, 128)               65666     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_2  (None, 2)                 516       \n",
            "=================================================================\n",
            "Total params: 4,905,893\n",
            "Trainable params: 2,453,218\n",
            "Non-trainable params: 2,452,675\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ATTg-CygHT-"
      },
      "source": [
        "Training our pruned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK6IFzKObSTl",
        "outputId": "9599ee19-dc60-404a-906e-74cb647a2e0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "logdir = tempfile.mkdtemp()\r\n",
        "\r\n",
        "callbacks = [\r\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\r\n",
        "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\r\n",
        "]\r\n",
        "\r\n",
        "model_for_pruning.fit(train_batches,\r\n",
        "                  batch_size=BATCH_SIZE, epochs=epochs, validation_data=validation_batches,\r\n",
        "                  callbacks=callbacks)\r\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "  6/582 [..............................] - ETA: 2:27 - loss: 0.4952 - accuracy: 0.7604WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0299s vs `on_train_batch_begin` time: 0.1047s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0299s vs `on_train_batch_begin` time: 0.1047s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0299s vs `on_train_batch_end` time: 0.0878s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0299s vs `on_train_batch_end` time: 0.0878s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "582/582 [==============================] - 40s 57ms/step - loss: 0.3889 - accuracy: 0.8271 - val_loss: 0.3826 - val_accuracy: 0.8263\n",
            "Epoch 2/2\n",
            "582/582 [==============================] - 35s 54ms/step - loss: 0.3448 - accuracy: 0.8500 - val_loss: 0.4064 - val_accuracy: 0.8267\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa785bc0d10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XwPLiq8gQlO"
      },
      "source": [
        "##Evaluate our models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdB4wtBsgYWN"
      },
      "source": [
        "Evaluate and save our models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8AGZU8rcgqj",
        "outputId": "da73d5ba-5fe2-4272-eaa6-103770a9ee51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "_, baseline_model_accuracy = model.evaluate(test_batches, verbose=0)\r\n",
        "\r\n",
        "_, keras_file = tempfile.mkstemp('.h5')\r\n",
        "tf.keras.models.save_model(model, keras_file, include_optimizer=False)\r\n",
        "print('Saved baseline model to:', keras_file)\r\n",
        "\r\n",
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(test_batches, verbose=0)\r\n",
        "\r\n",
        "print('Baseline test accuracy:', baseline_model_accuracy) \r\n",
        "print('Pruned test accuracy:', model_for_pruning_accuracy)\r\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved baseline model to: /tmp/tmpeb7ilfgh.h5\n",
            "Baseline test accuracy: 0.845227837562561\n",
            "Pruned test accuracy: 0.845227837562561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEjDNQQOhTBC"
      },
      "source": [
        "## Use TFlite to convert our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xSnyD0NSZPa",
        "outputId": "9950d2d2-9dc5-4159-fc28-501551acf267"
      },
      "source": [
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\r\n",
        "\r\n",
        "_, pruned_keras_file = tempfile.mkstemp('.h5')\r\n",
        "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\r\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)\r\n",
        "\r\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\r\n",
        "pruned_tflite_model = converter.convert()\r\n",
        "\r\n",
        "_, pruned_tflite_file = tempfile.mkstemp('.tflite')\r\n",
        "\r\n",
        "with open(pruned_tflite_file, 'wb') as f:\r\n",
        "  f.write(pruned_tflite_model)\r\n",
        "\r\n",
        "print('Saved pruned TFLite model to:', pruned_tflite_file)\r\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved pruned Keras model to: /tmp/tmp9al9srji.h5\n",
            "INFO:tensorflow:Assets written to: /tmp/tmp3x9_1dgv/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp3x9_1dgv/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved pruned TFLite model to: /tmp/tmphupiwp5y.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT9-pKCtfO0S"
      },
      "source": [
        "\r\n",
        "Applying a standard compression algorithm is necessary since the serialized weight matrices are the same size as they were before pruning. However, pruning makes most of the weights zeros, which is added redundancy that algorithms can utilize to further compress the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSNPPTD6fUq0"
      },
      "source": [
        "def get_gzipped_model_size(file):\r\n",
        "  # Returns size of gzipped model, in bytes.\r\n",
        "  import os\r\n",
        "  import zipfile\r\n",
        "\r\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\r\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\r\n",
        "    f.write(file)\r\n",
        "\r\n",
        "  return os.path.getsize(zipped_file)\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM4Zk5lJcXGv",
        "outputId": "f33912e7-3708-4570-ec4f-1267568e7881",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\r\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\r\n",
        "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of gzipped baseline Keras model: 2926328.00 bytes\n",
            "Size of gzipped pruned Keras model: 2926328.00 bytes\n",
            "Size of gzipped pruned TFlite model: 2871188.00 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afI0ar0yhZQV"
      },
      "source": [
        "## Excercice\r\n",
        "\r\n",
        "What can we see in term of the size and the accuracy of models ? *\r\n",
        "\r\n",
        "Try to create a smaller model from combining pruning and quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4IBgYCYooGD"
      },
      "source": [
        "## Clean Up\n",
        "\n",
        "Before running the next exercise, run the following cell to terminate the kernel and free memory resources:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0xuraEwfglj"
      },
      "source": [
        "import os, signal\r\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}