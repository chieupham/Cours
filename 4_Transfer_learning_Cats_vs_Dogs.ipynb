{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "4_Transfer_learning_Cats_vs_Dogs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2uKFqwx7Y5A"
      },
      "source": [
        "## Download Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLbtB3uGKPX"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akiAa5ypbBHg"
      },
      "source": [
        "## Download the neccessary data into the Colab Instance\r\n",
        "We will split our dataset into three subsets: training set, validation set and testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDRXk6VoccCs",
        "outputId": "bf0b4bf1-00c3-4f3a-9ca3-b7bce933db76"
      },
      "source": [
        "import tensorflow_datasets as tfds\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "\r\n",
        "BATCH_SIZE = 32\r\n",
        "IMAGE_SIZE = 224\r\n",
        "\r\n",
        "def format_image(image, label):\r\n",
        "    image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE)) / 255.0\r\n",
        "    return  image, label\r\n",
        "\r\n",
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\r\n",
        "    'cats_vs_dogs',\r\n",
        "    data_dir='/content/dataset/',\r\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\r\n",
        "    with_info=True,\r\n",
        "    shuffle_files=True, \r\n",
        "    as_supervised=True,\r\n",
        ")\r\n",
        "\r\n",
        "# tf.data.experimental.cardinality(raw_train_dataset).numpy()\r\n",
        "num_examples = metadata.splits['train'].num_examples\r\n",
        "num_classes = metadata.features['label'].num_classes\r\n",
        "print(\"Numbers of images: \", num_examples)\r\n",
        "print(\"Numbers of classes: \", num_classes)\r\n",
        "\r\n",
        "train_batches = raw_train.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE).prefetch(1)\r\n",
        "validation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(1)\r\n",
        "test_batches = raw_test.map(format_image).batch(1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numbers of images:  23262\n",
            "Numbers of classes:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEuJZ6OIbBHl"
      },
      "source": [
        "## Transfer learning\r\n",
        "We download the ResNet model and then take the feature extractor from the model. We define our last layer for cat vs dog classification and optimizer for our networks.\r\n",
        "\r\n",
        "ResNet is a family of network architectures for image classification, originally published by\r\n",
        "\r\n",
        "*Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun: \"Deep Residual Learning for Image Recognition\", 2015.*\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PixZ2s5QbYQ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74a739fa-3ed4-41e3-f5b2-9d31c7c47364"
      },
      "source": [
        "IMAGE_SIZE = (224, 224)\n",
        "FV_SIZE = 1280\n",
        "MODULE_HANDLE =\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
        "feature_extractor = hub.KerasLayer(MODULE_HANDLE,\n",
        "                                   input_shape=IMAGE_SIZE + (3,), \n",
        "                                   output_shape=[FV_SIZE],\n",
        "                                   trainable=False)\n",
        "\n",
        "feature_extractor = feature_extractor = hub.KerasLayer(MODULE_HANDLE,\n",
        "                                   input_shape=IMAGE_SIZE + (3,), \n",
        "                                   output_shape=[FV_SIZE],\n",
        "                                   trainable=False)\n",
        "model = tf.keras.Sequential([\n",
        "        feature_extractor,\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer_1 (KerasLayer)   (None, 1280)              2257984   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 2562      \n",
            "=================================================================\n",
            "Total params: 2,260,546\n",
            "Trainable params: 2,562\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GD6WwNXTqTQ"
      },
      "source": [
        "Then, we train our networks within only 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sRB_dmOTqwu",
        "outputId": "1f1ab24a-4402-4d39-bb66-a8fd695831b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.compile(optimizer='adam',\r\n",
        "                  loss='sparse_categorical_crossentropy',\r\n",
        "                  metrics=['accuracy'])\r\n",
        "\r\n",
        "EPOCHS = 5\r\n",
        "\r\n",
        "history = model.fit(train_batches,\r\n",
        "                 epochs=EPOCHS,\r\n",
        "                 validation_data=validation_batches)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "239/582 [===========>..................] - ETA: 25s - loss: 0.1880 - accuracy: 0.9099"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU0I-DCz7t0n"
      },
      "source": [
        "## Plotting the training process\r\n",
        "We plot the loss and accurracy of the training process with respect to the training set and the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sP0VBb50Kgn"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "# list all data in history\r\n",
        "print(history.history.keys())\r\n",
        "# summarize history for accuracy\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'valid'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "# summarize history for loss\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'valid'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAhmHnBn8RAm"
      },
      "source": [
        "## Run your Model\r\n",
        "\r\n",
        "Let's now take a look at actually running a prediction using the model. This code will test 100 images from the testing set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHkbVcCzBD5W"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "predictions = []\r\n",
        "\r\n",
        "# This will report how many iterations per second, where each\r\n",
        "# iteration is 100 predictions\r\n",
        "test_labels, test_imgs = [], []\r\n",
        "for img, label in tqdm(test_batches.take(100)):\r\n",
        "    predictions.append(model.predict(img))\r\n",
        "    \r\n",
        "    test_labels.append(label.numpy()[0])\r\n",
        "    test_imgs.append(img)\r\n",
        "\r\n",
        "\r\n",
        "# This will tell you how many of the predictions were correct\r\n",
        "score = 0\r\n",
        "for item in range(0,len(predictions)):\r\n",
        "  prediction=np.argmax(predictions[item])\r\n",
        "  label = test_labels[item]\r\n",
        "  if prediction==label:\r\n",
        "    score=score+1\r\n",
        "\r\n",
        "print(\"Out of 100 predictions I got \" + str(score) + \" correct\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d45Soe5VoFBv"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_2xbHg6oETp"
      },
      "source": [
        "CATS_VS_DOGS_SAVED_MODEL = \"./content/exp_saved_model\"\r\n",
        "tf.saved_model.save(model, CATS_VS_DOGS_SAVED_MODEL)\r\n",
        "# This will report back the file size in bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0ErcJjhqynr"
      },
      "source": [
        "import os\r\n",
        "# Get file size in bytes for a given model\r\n",
        "print(\"Model in Mb:\", os.path.getsize(CATS_VS_DOGS_SAVED_MODEL+'/saved_model.pb') / float(2**20))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0CnEYUYPw4o"
      },
      "source": [
        "##Exercice 1\r\n",
        "What do you notice in term of:\r\n",
        "\r\n",
        "* Model size \r\n",
        "* Training time\r\n",
        "* Accuracy\r\n",
        "\r\n",
        "compared to the network of the last notebook.\r\n",
        "\r\n",
        "How much is the size of the model in byte ?\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCr9EIttUVg6"
      },
      "source": [
        "##Exercice 2\r\n",
        "Now, we explore other feature extraction models to classify cats and dogs (by replacing MODULE_HANDLE =...) :\r\n",
        "\r\n",
        "* InceptionNet : https://tfhub.dev/google/imagenet/inception_v3/feature_vector/4 \\\\\r\n",
        "\r\n",
        "* MobileNetV2 : https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4 \r\n",
        "\r\n",
        "Inception V3 is a neural network architecture for image classification, originally published by\r\n",
        "\r\n",
        "> *Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna: \"Rethinking the Inception Architecture for Computer Vision\", 2015.*\r\n",
        "\r\n",
        "MobileNet V2 is a family of neural network architectures for efficient on-device image classification and related tasks, originally published by:\r\n",
        "\r\n",
        "> *Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen: \"Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation\", 2018.*\r\n",
        "\r\n",
        "There are many models that we can use, see here https://tfhub.dev/\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4IBgYCYooGD"
      },
      "source": [
        "## Clean Up\n",
        "\n",
        "Before running the next exercise, run the following cell to terminate the kernel and free memory resources:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "651IgjLyo-Jx"
      },
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}